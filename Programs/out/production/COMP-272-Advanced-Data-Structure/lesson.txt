* Any solution to a computational problem, improvements can be made in the following ways.
 * 1. research into the problem and use existing work (that was previously done)
 * 2. algorithmic improvement (clever solution)
 * 3. data structures
 *
 * After this,
 * 4. code level improvement (lot of little things or tricks you can do)
 * 5. (ignoring changing to a faster processor, or more memory)
 * 6. parallelism - which use of multiple cores on your computer

Algorithm analysis:
 - Big-O notation:
    1. records worst-case runtime as a mathematical function
    2. It is also asymptotic (size -> infinity)
    The relationship between input size n and runtime t is captured via O() notation.
    Operations that take units of time: multiplication, div, add, sub, comparison,
    assignment, evaluation a condition (boolean).
    If total time taken is independent of size of input n, then we call it O(1) algorithm (constant).
    For the same problem you can design algorithms with different runtime complexities.
    other examples: O(log(n)), O(n), O(n^2), O(2^n) ...
    examples:
    * outer loop 1 to n {
        inner loop 1 to n {
            simple operations (O(1) time)
        }
     }
        --> n * n = n^2 --> O(n^2) --> quadratic

    * if (X time condition) {
        = A
    } else {
        = B
    }
        --> time complexity of X + time of A or of B
        --> in this case, we take the max between time of A and B.
            --> so: time of X + max time between A and B.

    Some functions in ascending order:
    2/n, 37, sqrt(n), n, n.log(log(n)), n.log(n) (which is equal to n.log(n^2) in big-O notation),
    n.(log(n))^2, n^1.5, n^2, n^2.log(n), n^3, 2^(n/2) (which is the same as 2^(n) in big-O notation


    eg:
    function 1 (){
        for (i...n)
            function 2();
    }

    function 2 (){
        function 3 (); --> O(n.log(n))
    }
    --> Big-O of function 2 = Big-O of function 3. --> so, Big-O of function 1 is n.(n.log(n)) = (n^2).log(n)


    eg:
    // arraylist
    * get(i) --> O(1)
    * adding at the beginning --> O(n)
    * adding at the end --> O(1)
    // linked list
    * get(i) --> O(n) because we need to traverse the list
    * adding at the beginning --> O(1)
    * adding at the end --> O(1)
    * adding a value at an index --> O(n) because we need to traverse the list to find the index

    Note: suppose we want to print 5 elements from index 20 to 24. get(i) would be slow because it looks at the list
    from the beginning after each value. Instead, use list iterator.

    // stack
    * push() --> O(1)
    * pop() --> O(1)
    * isEmpty() --> O(1)
    * peek () --> O(1)

    // Queue
    *

    // Map
